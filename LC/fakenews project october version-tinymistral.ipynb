{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da3aecf4-4977-44df-976f-e7878685e620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.42.4\n",
      "  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (0.24.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (2024.7.24)\n",
      "Requirement already satisfied: requests in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from transformers==4.42.4) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.4) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.4) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from requests->transformers==4.42.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from requests->transformers==4.42.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from requests->transformers==4.42.4) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages (from requests->transformers==4.42.4) (2024.8.30)\n",
      "Downloading transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.43.3\n",
      "    Uninstalling transformers-4.43.3:\n",
      "      Successfully uninstalled transformers-4.43.3\n",
      "Successfully installed transformers-4.42.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install necessary libraries\n",
    "# !pip install transformers pandas scikit-learn torch\n",
    "# or conda defined environment\n",
    "#!pip install transformers==4.42.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78069c6-04c0-470a-b491-4f3948c21114",
   "metadata": {},
   "source": [
    "# Environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1345a3e6-cfac-4d3b-9a36-7c7f7cf726d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e733417-4bf0-4cd3-9e46-a7036b06ce30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda not available because the current PyTorch install was not built with Cuda enabled.\n",
      "Is Metal Performance Shader (mps) available? True\n",
      "Is Cuda available? False\n"
     ]
    }
   ],
   "source": [
    "# If Metal Performance Shader (mps) is not available tell me:\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "# If mps is available directly put it on the device.\n",
    "else:\n",
    "    device = torch.device(\"mps\")\n",
    ",\n",
    "# If cuda (nvidia gpu) is not available tell me:\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Cuda not available because the current PyTorch install was not \"\n",
    "              \"built with Cuda enabled.\")\n",
    "   \n",
    "\n",
    "# If cuda is available directly put it on the device.\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    ",\n",
    "\n",
    "# If neither cuda and mps are available, set device to \"cpu\"\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Neither Cuda nor MPS are available\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    ",\n",
    "# Is mps available?\n",
    "\n",
    "mps_avail = torch.backends.mps.is_available() \n",
    "print(f\"Is Metal Performance Shader (mps) available? {mps_avail}\")\n",
    "\n",
    ",\n",
    "\n",
    "# Is mps available?\n",
    "\n",
    "cuda_avail = torch.cuda.is_available() \n",
    "print(f\"Is Cuda available? {cuda_avail}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49b3e742-f3e9-4080-94a9-8423ec5ade60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu device:  mps\n"
     ]
    }
   ],
   "source": [
    "print(\"gpu device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "584daca4-1cc6-4be5-a64c-f274174d32c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7f07b50-8f29-4475-8376-728ca909daf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWe could also use pytorch_lightning package\\ntry:\\n    import pytorch_lightning as pl\\nexcept ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\\n    !pip install --quiet pytorch-lightning>=1.5\\n    import pytorch_lightning as pl\\n\\npl.seed_everything(42)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting seed:\n",
    "\n",
    "def set_seed_fun(seed_number: int):\n",
    "    np.random.seed(seed_number)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed_number)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.manual_seed(seed_number)\n",
    "        torch.cuda.manual_seed(seed_number)\n",
    "        torch.cuda.manual_seed_all(seed_number)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchark = False\n",
    "\n",
    "set_seed_fun(42)\n",
    "\"\"\"\n",
    "We could also use pytorch_lightning package\n",
    "try:\n",
    "    import pytorch_lightning as pl\n",
    "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
    "    !pip install --quiet pytorch-lightning>=1.5\n",
    "    import pytorch_lightning as pl\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334cc4a1-b355-4575-a6a3-6447b3a82e66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dc6ae47-93f1-40a0-9295-3c75928120c7",
   "metadata": {},
   "source": [
    "# Function definitions\n",
    "\n",
    "## Embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61579e8d-dc80-4910-9374-8911c5c95107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate embeddings with RoBERTa\n",
    "def generate_embeddings(texts, tokenizer, model):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length').to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb5108-17ca-499e-8ecd-b4933f1b2b18",
   "metadata": {},
   "source": [
    "## Class functions\n",
    "\n",
    "Definition of functions to:\n",
    "- Transform a data set into the good format\n",
    "- create a simple neural network architecture\n",
    "- create a funtion to transform seconds into a list of (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "423bc231-7992-40a8-aaaa-cc7920a72ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Dataset class for PyTorch\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c2b1910-63be-4ca7-ab7d-f441ad81c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.dropout1 = nn.Dropout(p=0.6)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.dropout2 = nn.Dropout(p=0.6)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2fedd9-c85c-4144-8dd5-139af39d891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sec2hms(ss):\n",
    "\t(hh, ss)=divmod(ss, 3600)\n",
    "\t(mm, ss)=divmod(ss, 60)\n",
    "\treturn (hh, mm, ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1eed2-7a53-4921-9d9f-cb35848dedbf",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "Definition of the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c424347-5b2d-42cd-829a-1a36e6dcf742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(X_train, y_train, X_val, y_val, input_size):\n",
    "    train_dataset = NewsDataset(X_train, y_train)\n",
    "    val_dataset = NewsDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = SimpleNN(input_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
    "\n",
    "    num_epochs = 8\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 2\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss}')\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc999217-4072-46d1-8e44-b05cd901fa66",
   "metadata": {},
   "source": [
    "# Training part\n",
    "\n",
    "## Loading of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99e07a94-fbf3-4d20-84bc-822e61682eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Google Drive\n",
    "# data_path = '/content/drive/My Drive/final_combined_dataset.csv'\n",
    "# df = pd.read_csv(data_path)\n",
    "df = pd.read_csv(\"final_combined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "051ea8e3-7436-46fa-9a31-b596466ce2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just in case of bug: to check where we are in the directories\n",
    "\n",
    "#import os\n",
    "\n",
    "#cwd = os.getcwd()  # Get the current working directory (cwd)\n",
    "#files = os.listdir(cwd)  # Get all the files in that directory\n",
    "#print(\"Files in %r: %s\" % (cwd, files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce1736-5492-4a67-9311-4294831030b0",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "befa178b-cbc7-4d7f-9c96-5cbb828470ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# benchmark beginning for embedding #\n",
    "#####################################\n",
    "time_start_embed = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d6dfdad-7d9b-45d5-8e23-758a97d87bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fb8f90a661471eb834aed907fb06ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/892 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b68a98c5e145a3aaf389aebfc31106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration.py:   0%|          | 0.00/7.13k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/dunzhang/stella_en_400M_v5:\n",
      "- configuration.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6afcd3c7414c7dbbd52535ce32bfcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling.py:   0%|          | 0.00/57.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/dunzhang/stella_en_400M_v5:\n",
      "- modeling.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37094c554574b4b9b6e58d7a7162496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "please install xformers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m roberta_tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta-base\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#embedding_model = RobertaModel.from_pretrained('roberta-base').to(device)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdunzhang/stella_en_400M_v5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:559\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m/Applications/anaconda3/envs/DL-torch-arm64/lib/python3.10/site-packages/transformers/modeling_utils.py:3788\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3782\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3783\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3784\u001b[0m )\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3787\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3788\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3791\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/dunzhang/stella_en_400M_v5/2aa5579fcae1c579de199a3866b6e514bbbf5d10/modeling.py:823\u001b[0m, in \u001b[0;36mNewModel.__init__\u001b[0;34m(self, config, add_pooling_layer)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings \u001b[38;5;241m=\u001b[39m NewEmbeddings(config)\n\u001b[0;32m--> 823\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mNewEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;241m=\u001b[39m NewPooler(config) \u001b[38;5;28;01mif\u001b[39;00m add_pooling_layer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;66;03m# Initialize weights and apply final processing\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/dunzhang/stella_en_400M_v5/2aa5579fcae1c579de199a3866b6e514bbbf5d10/modeling.py:696\u001b[0m, in \u001b[0;36mNewEncoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([NewLayer(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/dunzhang/stella_en_400M_v5/2aa5579fcae1c579de199a3866b6e514bbbf5d10/modeling.py:696\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mNewLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)])\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/dunzhang/stella_en_400M_v5/2aa5579fcae1c579de199a3866b6e514bbbf5d10/modeling.py:630\u001b[0m, in \u001b[0;36mNewLayer.__init__\u001b[0;34m(self, config, pack_qkv, use_memory_efficient_attention, attn_implementation)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_implementation \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meager\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    629\u001b[0m     use_memory_efficient_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention \u001b[38;5;241m=\u001b[39m \u001b[43mNEW_ATTENTION_CLASSES\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpack_qkv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpack_qkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_memory_efficient_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_memory_efficient_attention\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m NewGatedMLP(config)\n\u001b[1;32m    635\u001b[0m ln_class \u001b[38;5;241m=\u001b[39m LAYER_NORM[config\u001b[38;5;241m.\u001b[39mlayer_norm_type]\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/dunzhang/stella_en_400M_v5/2aa5579fcae1c579de199a3866b6e514bbbf5d10/modeling.py:451\u001b[0m, in \u001b[0;36mNewAttention.__init__\u001b[0;34m(self, config, pack_qkv, use_memory_efficient_attention)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_efficient_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m xops \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m xops\u001b[38;5;241m.\u001b[39mmemory_efficient_attention\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_memory_efficient_attention:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory_efficient_attention \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease install xformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39munpad_inputs:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_memory_efficient_attention, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munpad only with xformers\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: please install xformers"
     ]
    }
   ],
   "source": [
    "# Initialize RoBERTa for embeddings\n",
    "# Load model directly\n",
    "\n",
    "\n",
    "#roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "#embedding_model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
    "#embedding_model = AutoModel.from_pretrained(\"dunzhang/stella_en_400M_v5\", trust_remote_code=True)\n",
    "\n",
    "# Load model directly\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"reciprocate/tiny-mistral\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"reciprocate/tiny-mistral\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4ab2b-ee2a-4ce9-9b20-c457b0b0ac2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df['Text'].values.astype(str)\n",
    "y = (df['Label'] == 'fake').astype(int).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a4083-cada-4b09-8bc8-abd5a17e6c9c",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7d200-6a4e-4bb1-a166-05da53b3ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6a180-8158-4dba-9536-49ddd125244a",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30767c59-689d-4907-995e-6f96049dd229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "X_train_embeddings = generate_embeddings(X_train, roberta_tokenizer, embedding_model)\n",
    "X_val_embeddings = generate_embeddings(X_val, roberta_tokenizer, embedding_model)\n",
    "X_test_embeddings = generate_embeddings(X_test, roberta_tokenizer, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68c53d-0a6c-4f2e-99f6-1c85d0d26ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# benchmark ending for embedding    #\n",
    "#####################################\n",
    "time_end_embed = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebddb4-ac72-4d8e-9bc7-164b69619894",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511da9a-0124-4288-82a6-7a36576ad514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# benchmark beginning for modeling  #\n",
    "#####################################\n",
    "time_start_model = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5cc52f-0ddb-4024-824f-4365e19650b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding size\n",
    "input_size = 768  # Embedding size generated by RoBERTa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40991ebd-b228-4ebe-ae91-f750a75ce5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train_model(X_train_embeddings, y_train, X_val_embeddings, y_val, input_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43732b1f-cdf3-450d-a147-0349d3fc86be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# benchmark ending for modeling     #\n",
    "#####################################\n",
    "time_end_model = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076dd75-8f15-44d5-9bdd-4f15e858f39b",
   "metadata": {},
   "source": [
    "## Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d3f95-98e6-445b-8e1e-27c485679b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for X_batch in torch.tensor(X_test_embeddings, dtype=torch.float32).to(device):\n",
    "        outputs = model(X_batch.unsqueeze(0))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.append(predicted.cpu().numpy()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec37c8e-621e-46c8-a1d3-9badc7dd37e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred, target_names=[\"real\", \"fake\"])\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "print(f\"Classification Report:{class_report}\")\n",
    "print(f\"Confusion Matrix:{conf_matrix}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd422e1-9c39-43a3-9caa-511491cd9341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"real\", \"fake\"], yticklabels=[\"real\", \"fake\"])\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad31460-8f94-4de2-be81-aa55e8196ebb",
   "metadata": {},
   "source": [
    "## Benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc1ba3-f5f4-4723-bd6e-bbead42ddd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#          Benchmark results        #\n",
    "#####################################\n",
    "# calculating the performances\n",
    "embedding_duration = time_end_embed - time_start_embed\n",
    "modeling_duration = time_end_model - time_start_model\n",
    "\n",
    "# formating\n",
    "embedding_duration_hms = sec2hms(embedding_duration)\n",
    "modeling_duration_hms = sec2hms(modeling_duration)\n",
    "\n",
    "# printing the embedding and modeling performances\n",
    "\n",
    "print(f'Embedding duration : {embedding_duration_hms[0]:.0f}:{embedding_duration_hms[1]:.0f}:{embedding_duration_hms[2]:.3f}')\n",
    "print(f'Modeling duration : {modeling_duration_hms[0]:.0f}:{modeling_duration_hms[1]:.0f}:{modeling_duration_hms[2]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb71ef3-27fb-47bc-8c9b-b1d7dd9aeb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we should write a function with an if else condition for when a drive is in and where there's not\n",
    "# Save the trained model to Google Drive\n",
    "# final_model_path = '/content/drive/My Drive/fake_news_model_roberta3.pth'\n",
    "# torch.save(model.state_dict(), final_model_path)\n",
    "# print(f\"Model saved to: {final_model_path}\")\n",
    "torch.save(model.state_dict(), \"model.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0562ffcd-7920-4a03-adfa-92d93211fa31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ccc0f-9bae-4980-a701-6c95367f6731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
