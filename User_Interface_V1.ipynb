{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMHHEogWJDl5KO55/mlYlwV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anerol18/Fake_News_Detector_NLP_DeepLearning_Project/blob/main/User_Interface_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giM2Hney0a9B",
        "outputId": "dbf8e36f-f17d-4d1d-89d1-f9eeb29cf1c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-23 19:25:37--  https://github.com/Anerol18/Fake_News_Detector_NLP_DeepLearning_Project/raw/master/stella_model.pth\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://media.githubusercontent.com/media/Anerol18/Fake_News_Detector_NLP_DeepLearning_Project/master/stella_model.pth [following]\n",
            "--2024-10-23 19:25:37--  https://media.githubusercontent.com/media/Anerol18/Fake_News_Detector_NLP_DeepLearning_Project/master/stella_model.pth\n",
            "Resolving media.githubusercontent.com (media.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to media.githubusercontent.com (media.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 66097468 (63M) [application/octet-stream]\n",
            "Saving to: ‘stella_model.pth’\n",
            "\n",
            "stella_model.pth    100%[===================>]  63.04M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-10-23 19:25:42 (432 MB/s) - ‘stella_model.pth’ saved [66097468/66097468]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#load the saved model\n",
        "!wget https://github.com/Anerol18/Fake_News_Detector_NLP_DeepLearning_Project/raw/master/stella_model.pth\n"
      ]
    },
    {
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the SimpleNN class, with same achitecture used for the stella_model\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        # The input size for fc0 should match the saved model's input size\n",
        "        self.fc0 = nn.Linear(input_size, 3072)\n",
        "        self.dropout0 = nn.Dropout(p=0.6)\n",
        "        self.relu0 = nn.ReLU()\n",
        "        self.fc01 = nn.Linear(3072, 3072)\n",
        "        self.dropout01 = nn.Dropout(p=0.6)\n",
        "        self.relu01 = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(3072, 768)\n",
        "        self.dropout1 = nn.Dropout(p=0.6)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(768, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc0(x)\n",
        "        x = self.dropout0(x)\n",
        "        x = self.relu0(x)\n",
        "        x = self.fc01(x)\n",
        "        x = self.dropout01(x)\n",
        "        x = self.relu01(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load the model\n",
        "# Change input_size to 1536 to match the saved model's first layer input\n",
        "input_size = 1536\n",
        "model = SimpleNN(input_size)\n",
        "\n",
        "# Load the model weights\n",
        "model_path = 'stella_model.pth'  # Path to your model file\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# Set the model to evaluation"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQW_i7pr2gB3",
        "outputId": "821c4b93-372c-4517-bdc0-cc5a69b27c55"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1d5ca16530e9>:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess the input sentence\n",
        "def preprocess_input(sentence):\n",
        "    # Implement preprocessing steps\n",
        "    # Example: Tokenization and conversion to tensor\n",
        "    # This is just a placeholder; modify according to your model's requirements\n",
        "    tokenized_input = sentence.split()  # Simple split, replace with actual tokenization\n",
        "    # Convert to tensor (adjust the shape based on your model's input requirements)\n",
        "    input_tensor = torch.randn(1, input_size)  # Replace with actual data conversion\n",
        "    return input_tensor"
      ],
      "metadata": {
        "id": "fwdCG90G3YSu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make predictions\n",
        "def predict(sentence):\n",
        "    input_tensor = preprocess_input(sentence)  # Preprocess the input\n",
        "    with torch.no_grad():  # Disable gradient computation for inference\n",
        "        output = model(input_tensor)  # Get model output\n",
        "        _, predicted = torch.max(output, 1)  # Get the predicted class\n",
        "        return \"real\" if predicted.item() == 1 else \"fake\"  # Map class to label"
      ],
      "metadata": {
        "id": "dTfRVIvF3cHF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input\n",
        "user_input = input(\"Enter a sentence: \")\n",
        "prediction = predict(user_input)  # Make the prediction\n",
        "print(f\"The prediction is: {prediction}\")  # Display the result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkkTu4413qxU",
        "outputId": "17ed56d9-424a-46f2-cce1-73403d8ef916"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: Covid is a virus\n",
            "The prediction is: fake\n"
          ]
        }
      ]
    }
  ]
}